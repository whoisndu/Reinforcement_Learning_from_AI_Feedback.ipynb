{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoisndu/Reinforcement_Learning_from_AI_Feedback.ipynb/blob/main/Reinforcement_Learning_from_AI_Feedback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning from AI Feedback\n",
        "\n",
        "Fine-tunes a language model using natural language criteria for its sampled outputs.\n",
        "\n",
        "This notebook fine-tunes [EleutherAI](https://www.eleuther.ai/)'s [Pythia 160M](https://huggingface.co/EleutherAI/pythia-160m-deduped) language model using a zero-shot reward model derived from an instruct tuned language model ([Katherine Crowson's instruct fine-tune](https://huggingface.co/RiversHaveWings/minihf_evaluator_openllama_7b) of [OpenLLaMA 7B](https://huggingface.co/openlm-research/open_llama_7b)).\n",
        "\n",
        "The zero-shot reward model is obtained by asking the instruct model yes/no questions about the generations from the model that is being RLAIF tuned. It takes the logits for the first token of the response and forms a binary classifier logit as `log(p(yes) + p(neither) / 2) - log(p(no) + p(neither) / 2)`. It uses `log(sigmoid(logit))` (log probability of the \"yes\" class) as the reward. It uses weighted \"soft conjunctions\" of multiple binary classifier logits to fine-tune the model to satisfy multiple natural language criteria simultaneously.\n",
        "\n",
        "The gradient estimator is [DiCE](https://github.com/crowsonkb/dice-mc), a variant of REINFORCE. It uses a fixed strength KL penalty to constrain the fine-tuned model's distribution over tokens to not vary too far from the original model's.\n",
        "\n",
        "If you like this notebook you should check out [MiniHF](https://github.com/JD-P/minihf/), the language model fine-tuning and inference tool the code was originally written for.\n",
        "\n",
        "<small>Notebook by Katherine Crowson (crowsonkb@gmail.com, https://twitter.com/RiversHaveWings)\n",
        "<br>Sponsored by StabilityAI (https://twitter.com/stabilityai)\n",
        "<br>Copyright 2023 Katherine Crowson. Licensed under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0).</small>\n"
      ],
      "metadata": {
        "id": "lXDVdo_AVSii"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V09dXM8RVLyY"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "\n",
        "!pip install bitsandbytes dice-mc peft safetensors sentencepiece tokenizers transformers"
      ],
      "metadata": {
        "id": "6IQqn7CaWmBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import libraries\n",
        "\n",
        "from functools import partial\n",
        "import math\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "\n",
        "import dice_mc.torch as dice\n",
        "import peft\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "EWcVLujbbDre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define functions\n",
        "\n",
        "print = tqdm.external_write_mode()(print)\n",
        "\n",
        "\n",
        "def endless_range(start=0, step=1):\n",
        "    i = start\n",
        "    while True:\n",
        "        yield i\n",
        "        i += step\n",
        "\n",
        "\n",
        "def at_least_float32(tensor):\n",
        "    dtype = torch.promote_types(tensor.dtype, torch.float32)\n",
        "    return tensor.to(dtype)\n",
        "\n",
        "\n",
        "def logsumexp_scaled(a, b, return_sign=False, dim=None, keepdim=False):\n",
        "    \"\"\"Compute log(sum(b * exp(a))).\"\"\"\n",
        "    if dim is None:\n",
        "        dim = tuple(range(a.ndim))\n",
        "\n",
        "    a, b = torch.broadcast_tensors(a, b)\n",
        "    a = torch.where(b != 0, a, float(\"-inf\"))\n",
        "\n",
        "    a_max = torch.amax(a, dim=dim, keepdim=True)\n",
        "    a_max = torch.nan_to_num(a_max, 0.0, 0.0, 0.0)\n",
        "\n",
        "    tmp = b * torch.exp(a - a_max)\n",
        "\n",
        "    s = torch.sum(tmp, dim=dim, keepdim=keepdim)\n",
        "    if return_sign:\n",
        "        sgn = torch.sign(s)\n",
        "        s *= sgn\n",
        "    out = torch.log(s)\n",
        "\n",
        "    if not keepdim:\n",
        "        a_max = torch.squeeze(a_max, dim=dim)\n",
        "    out += a_max\n",
        "\n",
        "    if return_sign:\n",
        "        return out, sgn\n",
        "    else:\n",
        "        return out\n",
        "\n",
        "\n",
        "def soft_maximum(values, weights=None, tau=1.0, dim=None, keepdim=False):\n",
        "    if weights is None:\n",
        "        weights = torch.ones_like(values)\n",
        "    weights /= weights.sum(dim=dim, keepdim=True)\n",
        "    return logsumexp_scaled(values / tau, weights, dim=dim, keepdim=keepdim) * tau\n",
        "\n",
        "\n",
        "def soft_minimum(values, weights=None, tau=1.0, dim=None, keepdim=False):\n",
        "    if weights is None:\n",
        "        weights = torch.ones_like(values)\n",
        "    weights /= weights.sum(dim=dim, keepdim=True)\n",
        "    return -logsumexp_scaled(-values / tau, weights, dim=dim, keepdim=keepdim) * tau\n",
        "\n",
        "\n",
        "def get_scores_from_logits(logits, pos_tokens, neg_tokens):\n",
        "    logits = at_least_float32(logits[:, -1, :])\n",
        "    logits = F.log_softmax(logits, dim=-1)\n",
        "    pos = torch.logsumexp(logits[:, pos_tokens], dim=-1)\n",
        "    neg = torch.logsumexp(logits[:, neg_tokens], dim=-1)\n",
        "    rest = (1 - pos.exp() - neg.exp()).log()\n",
        "    return torch.logaddexp(pos, rest - math.log(2)) - torch.logaddexp(neg, rest - math.log(2))\n",
        "\n",
        "\n",
        "def find_token_for_string(tokenizer, prefix, s):\n",
        "    tok_prefix = tokenizer(prefix).input_ids\n",
        "    tok_prefix_s = tokenizer(prefix + s).input_ids\n",
        "    if tok_prefix_s[: len(tok_prefix)] != tok_prefix:\n",
        "        raise RuntimeError(f\"{prefix!r} tokens are not a prefix of {prefix + s!r} tokens\")\n",
        "    return tok_prefix_s[len(tok_prefix)]\n",
        "\n",
        "\n",
        "def find_tokens_for_strings(tokenizer, prefix, strings):\n",
        "    return sorted(set([find_token_for_string(tokenizer, prefix, s) for s in strings]))\n",
        "\n",
        "\n",
        "def make_get_scores(tokenizer, prefix):\n",
        "    pos_tokens = find_tokens_for_strings(tokenizer, prefix, [\"yes\", \"Yes\", \"YES\"])\n",
        "    neg_tokens = find_tokens_for_strings(tokenizer, prefix, [\"no\", \"No\", \"NO\"])\n",
        "    return partial(get_scores_from_logits, pos_tokens=pos_tokens, neg_tokens=neg_tokens)\n",
        "\n",
        "\n",
        "def kl_div_est(logp, logq):\n",
        "    \"\"\"Biased estimator of D_KL(P || Q) from log(p(x)) and log(q(x)), x sampled from p.\"\"\"\n",
        "    return torch.logaddexp(logp - logq, logq - logp) - math.log(2)\n",
        "\n",
        "\n",
        "def inv_cumsum(x):\n",
        "    \"\"\"Inverse of cumulative sum.\"\"\"\n",
        "    out = x.clone()\n",
        "    out[..., 1:] -= x[..., :-1]\n",
        "    return out\n",
        "\n",
        "\n",
        "def gradient_norm(params):\n",
        "    params = list(params)\n",
        "    total = params[0].new_tensor(0.0)\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            total += p.grad.pow(2).sum()\n",
        "    return total.sqrt()\n"
      ],
      "metadata": {
        "id": "DXfUnfhUbj05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define evaluator templates and prompts\n",
        "\n",
        "templates = [\n",
        "    \"\"\"Answer yes or no and only yes or no.\n",
        "\n",
        "=== Begin story ===\n",
        "{text}\n",
        "=== End story ===\n",
        "\n",
        "Does this story make the reader feel like crying?\"\"\",\n",
        "    \"\"\"Answer yes or no and only yes or no.\n",
        "\n",
        "=== Begin story ===\n",
        "{text}\n",
        "=== End story ===\n",
        "\n",
        "Is this story well-written and coherent?\"\"\",\n",
        "]\n",
        "weights = [1.0, 0.5]\n",
        "signs = [1, 1]\n",
        "\n",
        "\n",
        "def make_evaluator_prompts(texts):\n",
        "    return [[template.format(text=text) + \"<|end|>\" for text in texts] for template in templates]\n",
        "\n",
        "\n",
        "train_prompts = [\n",
        "    \"My cat is so cute, but\",\n",
        "    \"I was watching TV, and\",\n",
        "    \"She looked in the mirror and\",\n",
        "    \"Alice said, \\\"\",\n",
        "]\n",
        "\n",
        "eval_prompts = train_prompts"
      ],
      "metadata": {
        "id": "I7NCwCJ6cI3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training parameters\n",
        "\n",
        "#@markdown Batch size:\n",
        "bs = 12  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Number of tokens to sample per batch item:\n",
        "n_tokens = 48  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown KL penalty weight:\n",
        "#@markdown <br><small>Constrains the fine-tuned model to be close to the original model. The larger the KL penalty, the less it is allowed to deviate from the original model's distribution.</small>\n",
        "kl_weight = 1.0  #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Temperature for soft conjunction:\n",
        "#@markdown <br><small>Interpolates between the weighted mean of the reward components (evaluator templates) and their minimum. Higher temperature is more mean-like, lower is more minimum-like.</small>\n",
        "tau = 1.0  #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Save every this many steps:\n",
        "save_every = 250  #@param {type:\"integer\"}\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hNbkWo6JfEMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load evaluator model\n",
        "\n",
        "# Use small-shard safetensors version of openlm-research/open_llama_7b to be\n",
        "# able to load the model on non-high RAM Colab instances\n",
        "eval_model_name = \"RiversHaveWings/open_llama_7b_safetensors\"\n",
        "eval_adapter_name = \"RiversHaveWings/minihf_evaluator_openllama_7b\"\n",
        "\n",
        "print(\"Loading evaluator model tokenizer...\")\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(eval_adapter_name)\n",
        "eval_tokenizer.padding_side = \"left\"\n",
        "\n",
        "print(\"Loading evaluator base model...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "eval_model = AutoModelForCausalLM.from_pretrained(\n",
        "    eval_model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "print(\"Loading evaluator adapter...\")\n",
        "eval_model = peft.PeftModel.from_pretrained(eval_model, eval_adapter_name)\n",
        "eval_model.requires_grad_(False);\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "ceJddybicwXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load model to fine-tune\n",
        "\n",
        "model_name = \"EleutherAI/pythia-160m-deduped\"\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "peft_config = peft.LoraConfig(\n",
        "    peft.TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=32,\n",
        "    lora_alpha=8,\n",
        "    lora_dropout=0.0,\n",
        "    target_modules=[\n",
        "        \"attention.query_key_value\",\n",
        "        \"attention.dense\",\n",
        "        \"mlp.dense_h_to_4h\",\n",
        "        \"mlp.dense_4h_to_h\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"Initializing adapter...\")\n",
        "model = peft.get_peft_model(model, peft_config)\n",
        "model.train()\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "id": "AG2IUrohyt8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training loop\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "output_path = \"model\"\n",
        "\n",
        "train_inputs = tokenizer(train_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
        "eval_inputs = tokenizer(eval_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
        "input_n, input_len = train_inputs.input_ids.shape\n",
        "get_scores = make_get_scores(eval_tokenizer, \"<|end|>\")\n",
        "weights_ = torch.tensor(weights, device=device)[None]\n",
        "signs_ = torch.tensor(signs, device=device)[None]\n",
        "\n",
        "opt = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98))\n",
        "baseline = dice.EMABaseline(decay=0.98).to(device)\n",
        "baseline_kl = dice.EMABaseline(decay=0.98).to(device)\n",
        "\n",
        "\n",
        "for i in tqdm(endless_range()):\n",
        "    # Demo generations\n",
        "    if i % 50 == 0:\n",
        "        outputs = model.generate(\n",
        "            eval_inputs.input_ids,\n",
        "            attention_mask=eval_inputs.attention_mask,\n",
        "            do_sample=True,\n",
        "            min_new_tokens=n_tokens,\n",
        "            max_new_tokens=n_tokens,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            top_k=0,\n",
        "        )\n",
        "        texts = [tokenizer.decode(toks, skip_special_tokens=True) for toks in outputs]\n",
        "        print(\"======\")\n",
        "        print(\"\\n===\\n\".join(textwrap.fill(text, width=80) for text in texts))\n",
        "        print(\"======\")\n",
        "\n",
        "    # Save model\n",
        "    if i > 0 and i % save_every == 0:\n",
        "        print(\"Saving model...\")\n",
        "        tokenizer.save_pretrained(output_path)\n",
        "        model.save_pretrained(output_path, safe_serialization=True)\n",
        "\n",
        "    # Sample from training prompts\n",
        "    indices = torch.randint(0, input_n, [bs], device=device)\n",
        "    tokens = model.generate(\n",
        "        train_inputs.input_ids[indices],\n",
        "        attention_mask=train_inputs.attention_mask[indices],\n",
        "        do_sample=True,\n",
        "        min_new_tokens=n_tokens,\n",
        "        max_new_tokens=n_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_k=0,\n",
        "    )\n",
        "\n",
        "    # Get logits with grad for backprop\n",
        "    attention_mask = torch.cat(\n",
        "        [train_inputs.attention_mask[indices], torch.ones_like(tokens[:, input_len:])], dim=1\n",
        "    )\n",
        "    outputs = model(tokens, attention_mask=attention_mask)\n",
        "\n",
        "    # Create stochastic nodes\n",
        "    logp = dice.logp_categorical(outputs.logits[:, input_len - 1 : -1], tokens[:, input_len:])\n",
        "    logp_sum = torch.sum(logp, dim=1)\n",
        "    logp_cumsum = torch.cumsum(logp, dim=1)\n",
        "\n",
        "    # Get original model logits and compute KL penalties\n",
        "    with torch.no_grad(), model.disable_adapter():\n",
        "        outputs_orig = model(tokens, attention_mask=attention_mask)\n",
        "    logp_orig = dice.logp_categorical(outputs_orig.logits[:, input_len - 1 : -1], tokens[:, input_len:])\n",
        "    logp_orig_cumsum = torch.cumsum(logp_orig, dim=1)\n",
        "    kls = inv_cumsum(kl_div_est(logp_cumsum.detach(), logp_orig_cumsum.detach()))\n",
        "\n",
        "    # Compute rewards using evaluator model\n",
        "    texts = [tokenizer.decode(t, skip_special_tokens=True) for t in tokens]\n",
        "    prompts_all = make_evaluator_prompts(texts)\n",
        "    inputs_all = [\n",
        "        eval_tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
        "        for prompts in prompts_all\n",
        "    ]\n",
        "    with torch.no_grad():\n",
        "        outputs_all = [\n",
        "            eval_model(inputs.input_ids, attention_mask=inputs.attention_mask)\n",
        "            for inputs in inputs_all\n",
        "        ]\n",
        "    scores = torch.stack([get_scores(outputs.logits) for outputs in outputs_all], dim=1)\n",
        "    scores = soft_minimum(scores * signs_, weights_, tau=tau, dim=1)\n",
        "\n",
        "    # Create cost nodes and baselines, then backprop\n",
        "    losses_main = -F.logsigmoid(scores)\n",
        "    losses_main = dice.cost_node(losses_main, [logp_sum])\n",
        "    losses_main += baseline(losses_main, [logp_sum])\n",
        "    losses_kl = kls * kl_weight\n",
        "    losses_kl = dice.cost_node(losses_kl, [logp_cumsum])\n",
        "    losses_kl += baseline_kl(losses_kl, [logp_cumsum])\n",
        "    loss_main = losses_main.mean()\n",
        "    loss_kl = losses_kl.mean()\n",
        "    loss = loss_main + loss_kl\n",
        "    loss.backward()\n",
        "\n",
        "    # Print metrics\n",
        "    grad_norm = gradient_norm(model.parameters())\n",
        "    print(f\"step: {i}, loss: {loss.item():g}, main: {loss_main.item():g}, kl: {loss_kl.item():g}, grad norm: {grad_norm.item():g}\")\n",
        "\n",
        "    # Take an optimizer step\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n"
      ],
      "metadata": {
        "id": "0NFQoa1veHMx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}